{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Oversampled Data</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Original Data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Test Balanced Accuracy</th>\n",
       "      <th>Test F1 Score</th>\n",
       "      <th>Test ROC AUC</th>\n",
       "      <th>Test G-Mean</th>\n",
       "      <th>Test Balanced Accuracy</th>\n",
       "      <th>Test F1 Score</th>\n",
       "      <th>Test ROC AUC</th>\n",
       "      <th>Test G-Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abalone19</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ecoli1</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flare-F</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>poker-8-9_vs_5</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeast5</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yeast6</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Oversampled Data                             \\\n",
       "          Dataset Test Balanced Accuracy Test F1 Score Test ROC AUC   \n",
       "0       abalone19                  0.898         0.476        0.945   \n",
       "1          ecoli1                  0.779         0.632        0.779   \n",
       "2         flare-F                  0.734         0.499        0.807   \n",
       "3  poker-8-9_vs_5                  0.824         0.077        0.824   \n",
       "4          yeast5                  0.972         0.500        0.972   \n",
       "5          yeast6                  0.823         0.607        0.929   \n",
       "\n",
       "                       Original Data                                         \n",
       "  Test G-Mean Test Balanced Accuracy Test F1 Score Test ROC AUC Test G-Mean  \n",
       "0       0.845                  0.500         0.498        0.839       0.707  \n",
       "1       0.778                  0.798         0.667        0.798       0.797  \n",
       "2       0.726                  0.500         0.490        0.225       0.707  \n",
       "3       0.805                  0.667         0.500        0.667       0.577  \n",
       "4       0.972                  0.828         0.138        0.828       0.809  \n",
       "5       0.859                  0.618         0.634        0.854       0.781  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory containing the datasets\n",
    "directory = '.'\n",
    "\n",
    "# Initialize an empty list to store the data for the final DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Function to extract the best row based on Test G-Mean and return specific metrics\n",
    "def get_best_metrics(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    best_row = df.loc[df['Test G-Mean'].idxmax()]\n",
    "    # Extract desired metrics and round to 3 decimal places\n",
    "    metrics = {\n",
    "        'Test Balanced Accuracy': round(best_row['Test Balanced Accuracy'], 3),\n",
    "        'Test F1 Score': round(best_row['Test F1 Score'], 3),\n",
    "        'Test ROC AUC': round(best_row['Test ROC AUC'], 3),\n",
    "        'Test G-Mean': round(best_row['Test G-Mean'], 3)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Loop through directories to find datasets and their corresponding result files\n",
    "for folder_name in os.listdir(directory):\n",
    "    dataset_path = os.path.join(directory, folder_name)\n",
    "    if os.path.isdir(dataset_path):  # Check if it's a directory\n",
    "        # Identify result files for the dataset\n",
    "        results_file = os.path.join(dataset_path, f\"{folder_name}_results.csv\")\n",
    "        original_results_file = os.path.join(dataset_path, f\"{folder_name}_original_results.csv\")\n",
    "        \n",
    "        # Ensure both files exist\n",
    "        if os.path.exists(results_file) and os.path.exists(original_results_file):\n",
    "            # Extract metrics for oversampled and original data\n",
    "            oversampled_metrics = get_best_metrics(results_file)\n",
    "            original_metrics = get_best_metrics(original_results_file)\n",
    "            \n",
    "            # Add a row for the current dataset to the comparison data\n",
    "            comparison_data.append([\n",
    "                folder_name,  # Dataset name\n",
    "                oversampled_metrics['Test Balanced Accuracy'],\n",
    "                oversampled_metrics['Test F1 Score'],\n",
    "                oversampled_metrics['Test ROC AUC'],\n",
    "                oversampled_metrics['Test G-Mean'],\n",
    "                original_metrics['Test Balanced Accuracy'],\n",
    "                original_metrics['Test F1 Score'],\n",
    "                original_metrics['Test ROC AUC'],\n",
    "                original_metrics['Test G-Mean']\n",
    "            ])\n",
    "\n",
    "# Create the final DataFrame\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    ('', 'Dataset'),\n",
    "    ('Oversampled Data', 'Test Balanced Accuracy'),\n",
    "    ('Oversampled Data', 'Test F1 Score'),\n",
    "    ('Oversampled Data', 'Test ROC AUC'),\n",
    "    ('Oversampled Data', 'Test G-Mean'),\n",
    "    ('Original Data', 'Test Balanced Accuracy'),\n",
    "    ('Original Data', 'Test F1 Score'),\n",
    "    ('Original Data', 'Test ROC AUC'),\n",
    "    ('Original Data', 'Test G-Mean')\n",
    "], names=['', ''])\n",
    "\n",
    "# Create the DataFrame with the corrected columns\n",
    "comparison_df = pd.DataFrame(comparison_data, columns=columns)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "comparison_df.reset_index(drop=True, inplace=True)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
